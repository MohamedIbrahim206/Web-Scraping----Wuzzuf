{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8103fe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tkinter import *\n",
    "from tkinter import messagebox\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tkinter import messagebox\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d6f2cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page num 41\n",
      "15\n",
      "1\n",
      "Page switched\n",
      "page num 41\n",
      "endeed\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "root = Tk()\n",
    "root.title(\"Wuzzuf scraping \")\n",
    "root.geometry('700x600')\n",
    "root.resizable(False , False)\n",
    "root.config(background='whitesmoke')\n",
    "job = StringVar()\n",
    "num_pages = StringVar()\n",
    "df  = \"\" \n",
    "\n",
    "photo = PhotoImage(file = \"E:\\\\Samsung Innovation Campus\\\\Tasks\\Web Scraping\\\\wuzzuf-squarelogo-1586535330103.png\" )\n",
    "panel = Label(root , image = photo)\n",
    "panel.place(x= 230 , y =50)\n",
    "\n",
    "\n",
    "lb1 = Label(text = \"Enter a job name   :\" , font = (\"Arial\" , 13 ) ,bg = 'whitesmoke' )\n",
    "lb1.place(x =130 , y =320)\n",
    "\n",
    "ent = Entry(width = '25'  ,textvariable = job)\n",
    "ent.place(x = 300 , y = 327)\n",
    "\n",
    "lb1 = Label(text = \"Enter the number of pages :\" , font = (\"Arial\" , 13 ) ,bg = 'whitesmoke' )\n",
    "lb1.place(x =80 , y =380)\n",
    "\n",
    "ent = Entry(width = '25' , textvariable =num_pages )\n",
    "ent.place(x = 300 , y = 387)\n",
    "\n",
    "\n",
    "\n",
    "# class wuzzuf:\n",
    "# #     def init(self):\n",
    "# #         self.x = 'Welcome to Wuzzuf'\n",
    "\n",
    "def job2():\n",
    "    \n",
    "    job_name = job.get()\n",
    "    n_of_pages = num_pages.get()\n",
    "    \n",
    "    titles=[]\n",
    "    company=[]\n",
    "    location=[]\n",
    "    skill=[]\n",
    "    country=[]\n",
    "    new_location=[]\n",
    "    final_loc=[]\n",
    "    date=[]\n",
    "    work_hours=[]\n",
    "    twon=[]\n",
    "    city=[]\n",
    "    final_skill=[]\n",
    "    experience=[]\n",
    "    experience_years=[]\n",
    "    skill2=[]\n",
    "    links=[]\n",
    "    job_des=[]\n",
    "    label=[]\n",
    "    education_level=[]\n",
    "    job_req=[]\n",
    "    vacancies=[]\n",
    "    num=0\n",
    "    job_name=job_name.lower()\n",
    "    name=job_name.split(' ')\n",
    "    n_of_pages=int(n_of_pages)-1\n",
    "#     for i in range (3):\n",
    "#         num=0\n",
    "    url='https://wuzzuf.net/search/jobs/?a=navbg&q='+str(name[0])+'%20'+str(name[1])+'&start='+str(num)\n",
    "    result=requests.get(url)\n",
    "    src=result.content\n",
    "    soup=BeautifulSoup(src,'lxml')\n",
    "    page_limit=int(soup.find('strong').text)\n",
    "    if(int(n_of_pages)>page_limit//15):\n",
    "        print('Sorry,Limited Pages,maximum pages you can get is ',(page_limit//15)+1)\n",
    "    else:\n",
    "        while True:\n",
    "            \n",
    "\n",
    "            print('page num',page_limit)\n",
    "            if(num>int(n_of_pages)):\n",
    "                print('endeed')\n",
    "                break\n",
    "            job_titles=soup.find_all('h2',{'class':'css-m604qf'})\n",
    "            company_names=soup.find_all('a',{'class':'css-17s97q8'})\n",
    "            locations=soup.find_all('span',{'class':'css-5wys0k'})\n",
    "            time=soup.find_all('div',{'class':'css-1lh32fc'})\n",
    "            skill_descr=soup.find_all('div',{'class':'css-y4udm8'})\n",
    "            posted_time=soup.find_all('div',{'class':'css-d7j1kk'})\n",
    "            print(len(job_titles))\n",
    "            for j in range (len(job_titles)):\n",
    "                titles.append(job_titles[j].text)\n",
    "                links.append('https://wuzzuf.net/'+job_titles[j].find('a').attrs['href'])\n",
    "                company.append(company_names[j].text[:-2])\n",
    "                location.append(locations[j].text)\n",
    "                date.append(posted_time[j].text)\n",
    "                work_hours.append(time[j].text)\n",
    "                skill.append(skill_descr[j].text)\n",
    "                label.append(job_name)\n",
    "            num+=1\n",
    "            print(num)\n",
    "            print('Page switched')\n",
    "        for link in links:\n",
    "#             print(link)\n",
    "            try:\n",
    "                headers = {\n",
    "                        \"Connection\": \"keep-alive\",\n",
    "                        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\",\n",
    "                    }\n",
    "                result2=requests.get(link,headers=headers,timeout=5)\n",
    "                src2=result2.content\n",
    "                soup2=BeautifulSoup(src2,'lxml')\n",
    "            #     salaries=soup2.find('span',{'class':'css-4xky9y'})\n",
    "                description=soup2.find(\"span\", attrs={\"itemprop\": \"description\"})\n",
    "                edu_level=soup2.find(\"dd\", attrs={\"class\": \"requirement-content\"})\n",
    "                job_r=soup2.find(\"span\", attrs={\"itemprop\": \"responsibilities\"})\n",
    "                job_des.append(description.text)\n",
    "                education_level.append(edu_level.text)\n",
    "                job_req.append(job_r.text)\n",
    "                print(\"Done\")\n",
    "            except:\n",
    "                job_des.append('No Job Description')\n",
    "                job_req.append('No Job Requirement')\n",
    "                \n",
    "            \n",
    "        for i in range (len(location)):\n",
    "                country.append(location[i].split(',')[-1])\n",
    "                new_location.append(location[i].split(',')[:-1])\n",
    "        for i in range(len(new_location)):\n",
    "            final_loc.append(' '.join(new_location[i]))\n",
    "        \n",
    "        for i in range(len(skill)):\n",
    "            skill[i]=skill[i].lstrip(work_hours[i])\n",
    "            date[i]=date[i].replace(location[i],'')\n",
    "            date[i]=date[i].replace(company[i],'')        \n",
    "        for i in range (len(date)):\n",
    "            date[i]=date[i][3:]\n",
    "        \n",
    "        for i in range (len(titles)):\n",
    "            skill[i].split('路')\n",
    "            skill[i]=skill[i].replace('Yrs of Exp','')\n",
    "            experience.append(skill[i].split('路')[0].strip())\n",
    "            experience_years.append(skill[i].strip().split('路')[1])\n",
    "            skill2.append(skill[i].split('路')[2:])\n",
    "        for i in range(len(new_location)):\n",
    "            final_skill.append(' '.join(skill2[i]))\n",
    "            if(len(location[i].split(','))==2):\n",
    "                twon.append('No twon avaliable')\n",
    "                city.append(location[i].split(',')[0].strip())\n",
    "            else:\n",
    "                twon.append(location[i].split(',')[0].strip())\n",
    "                city.append(location[i].split(',')[1].strip())\n",
    "                \n",
    "        global  df        \n",
    "        df=pd.DataFrame({'job_name':label,'job_title':titles,'company_name':company,'town':twon,'city':city,'country':country,'posted_date':date,'work_hours':work_hours,'career_level':experience,'experience_yreas':experience_years,'job_description':job_des,'job_requirement':job_req,'skills':final_skill})\n",
    "        messagebox.showinfo(\"Done\" )\n",
    "        return df\n",
    "\n",
    "    \n",
    "def save ():\n",
    "    filepath = Path(\"E:\\Samsung Innovation Campus\\Tasks\\Web Scraping\\\\data1.csv\")  \n",
    "    # filepath.parent.mkdir(parents=True, exist_ok=True) \n",
    "    global  df        \n",
    "\n",
    "    df.to_csv(filepath,index=False)\n",
    "    print(\"OK\")\n",
    "\n",
    "\n",
    "bt1 = Button(text = \"Scrap\" , font = (\"Arial\" , 15 ) ,bg = 'white' , fg = 'black',width = '11'  ,command = job2 )\n",
    "bt1.place(x =200 , y = 450)\n",
    "\n",
    "bt2 = Button(text = \"Save\" , font = (\"Arial\" , 15 ) ,bg = 'white' , fg = 'black' ,width = '11'  , command =save)\n",
    "bt2.place(x =400 , y = 450)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f64ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
